#pragma kernel ScreenSpaceReflectionMarching

#include "../../../ShaderLibrary/Core.hlsl"
#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/GlobalSamplers.hlsl"

#define SSR_TRACE_EPS               0.000488281f // 2^-11, should be good up to 4K

CBUFFER_START(ScreenSpaceReflectionShaderVariables)
    float4x4 _InvViewProjection;
    float4x4 _HistoryReprojection;
    float4 _ScreenSize;
    int _SsrStencilBit;
CBUFFER_END

Texture2D<uint2> _StencilBuffer;
Texture2D<float> _DepthPyramidTexture;
Texture2D<float3> _SsrHistoryColorTexture;
Texture2D<float2> _SsrHitPointTexture;

RWTexture2D<float4> _SsrLightingTexture;

// World space position reconstruction
float3 ReconstructWorldSpacePosition(float2 uv, float deviceDepth, float4x4 invViewProjMatrix)
{
    // 构建裁剪空间坐标
    float4 positionCS = float4(uv * 2.0 - 1.0, deviceDepth, 1.0);

    // 如果是 Y-down 的平台, 反转裁剪空间 y 坐标
#if UNITY_UV_STARTS_AT_TOP
    positionCS.y = -positionCS.y;
#endif

    // 通过 VP 的逆矩阵将裁剪空间坐标变换到世界空间坐标
    float4 hpositionWS = mul(invViewProjMatrix, positionCS);

    // 透视除法
    return hpositionWS.xyz / hpositionWS.w;
}

// World space normal reconstruction
float3 ReconstructWorldSpaceNormal(float2 uv, float3 origin, float2 texelSize)
{
    float2 dx = float2(texelSize.x, 0.0);
    float2 dy = float2(0.0, texelSize.y);

    float2 uvdx = uv + dx;
    float2 uvdy = uv + dy;
    float depth0 = _DepthPyramidTexture.SampleLevel(sampler_PointClamp, uvdx, 0).r;
    float depth1 = _DepthPyramidTexture.SampleLevel(sampler_PointClamp, uvdy, 0).r;
    float3 px = ComputeWorldSpacePosition(uvdx, depth0, _InvViewProjection);
    float3 py = ComputeWorldSpacePosition(uvdy, depth1, _InvViewProjection);

    float3 dpdx = px - origin;
    float3 dpdy = py - origin;

    return normalize(cross(dpdy, dpdx));
}

// Performs fading at the edge of the screen.
float EdgeOfScreenFade(float2 coordNDC, float fadeRcpLength)
{
    float2 coordCS = coordNDC * 2 - 1;
    float2 t = Remap10(abs(coordCS), fadeRcpLength, fadeRcpLength);
    return Smoothstep01(t.x) * Smoothstep01(t.y);
}

[numthreads(8, 8, 1)]
void ScreenSpaceReflectionMarching(uint3 dispatchThreadId : SV_DispatchThreadID)
{
    // 当前纹素的屏幕坐标
    uint2 positionSS = dispatchThreadId.xy;
    // 读取 stencil buffer
    uint stencilValue = GetStencilValue(_StencilBuffer.Load(int3(positionSS, 0)));
    // 对于不需要计算 SSR 的纹素, 直接返回 0.0
    bool doesntReceiveSSR = (stencilValue & _SsrStencilBit) == 0;
    UNITY_BRANCH
    if (doesntReceiveSSR)
    {
        _SsrLightingTexture[positionSS] = float4(0.0, 0.0, 0.0, 0.0);
        return;
    }

    // 当前纹素中心的 uv 坐标
    float2 uv = positionSS * _ScreenSize.zw + (0.5 * _ScreenSize.zw);
    // 采样深度值
    float deviceDepth = _DepthPyramidTexture.SampleLevel(sampler_PointClamp, uv, 0).r;
    // 重建世界空间坐标
    float3 positionWS = ReconstructWorldSpacePosition(uv, deviceDepth, _InvViewProjection);
    // 重建世界空间法线向量
    float3 N = ReconstructWorldSpaceNormal(uv, positionWS, _ScreenSize.zw);
    // 计算世界空间下的反射光线
    float3 V = GetWorldSpaceNormalizeViewDir(positionWS);
    float3 R = reflect(-V, N);

    // 屏幕坐标中, 反射光线步进的起始坐标, 也就是当前纹素的中心位置
    float3 rayOrigin = float3(positionSS + 0.5, deviceDepth);

    float3 reflPosWS = positionWS + R;
    float3 reflPosNDC = ComputeNormalizedDeviceCoordinatesWithZ(reflPosWS, UNITY_MATRIX_VP);
    float3 reflPosSS = float3(reflPosNDC.xy * _ScreenSize.xy, reflPosNDC.z);
    // 屏幕坐标中, 反射光线的方向向量
    float3 rayDir = reflPosSS - rayOrigin;

    float3 rcpRayDir = rcp(rayDir);

    int2 rayStep = int2(rcpRayDir.x >= 0 ? 1 : 0,
                        rcpRayDir.y >= 0 ? 1 : 0);

    // 每一次步进的 xyz 方向
    float3 raySign = float3(rcpRayDir.x >= 0 ? 1 : -1,
                             rcpRayDir.y >= 0 ? 1 : -1,
                             rcpRayDir.z >= 0 ? 1 : -1);

    // 反射光线是不是朝向相机 (因为反向 Z, 所以越靠近相机的位置深度值越大)
    bool rayTowardsEye = rcpRayDir.z >= 0;

    // 跳过处于 far plane 的纹素 (例如背景天空盒)
    bool killRay = deviceDepth == UNITY_RAW_FAR_CLIP_VALUE;
    // 跳过反射光线超出视锥的纹素
    killRay = killRay || (reflPosSS.z <= 0);
    // 跳过相机看不到的纹素
    killRay = killRay || (dot(N, V) <= 0);
    // 跳过反射光线是远离相机的纹素
    killRay = killRay || rayTowardsEye;
    if (killRay)
    {
        _SsrLightingTexture[positionSS] = float4(0.0, 0.0, 0.0, 0.0);
        return;
    }

    // 计算步进的最远距离, 最远距离也就是到屏幕边缘
    float tMax;
    {
        // Shrink the frustum by half a texel for efficiency reasons.
        const float halfTexel = 0.5;

        // 计算 xyz 的最远距离
        float3 bounds;
        // 步进方向的 xy 分量如果为正, 则 xy 的最远距离就是 ScreenSize.xy - 0.5 , 反之则是 0.5
        bounds.x = (rcpRayDir.x >= 0) ? _ScreenSize.x - halfTexel : halfTexel;
        bounds.y = (rcpRayDir.y >= 0) ? _ScreenSize.y - halfTexel : halfTexel;
        // 步进方向的 z 分量如果为正, 则 z 的最远距离就是近平面 1 , 反之则是 6.103515625e-5 , 这个值可以不用太小, 节省性能
        bounds.z = (rcpRayDir.z >= 0) ? 1 : 0.0001;

        // 3 个分量距离中取最小作为最大步进距离
        float3 dist = bounds * rcpRayDir - (rayOrigin * rcpRayDir);
        tMax = Min3(dist.x, dist.y, dist.z);
    }

    // 每次步进与起点的距离, 初始值从下一个纹素开始, 避免初始值为 0 造成自相交
    float t;
    {
        float2 dist = abs(0.5 * rcpRayDir.xy);
        t = min(dist.x, dist.y);
    }

    // 最大步进次数
    const int _SsrIterLimit = 64;
    // 场景中几何体的厚度, 是定义在线性深度值中的偏移. 用于在光线步进时是判断否与几何体相交
    // thickness, thickness_scale, thickness_bias 是怎么定义的具体可以看下面 HDRP 中的注释推导:
    // We define the depth of the base as the depth value as:
    // b = DeviceDepth((1 + thickness) * LinearDepth(d))
    // b = ((f - n) * d + n * (1 - (1 + thickness))) / ((f - n) * (1 + thickness))
    // b = ((f - n) * d - n * thickness) / ((f - n) * (1 + thickness))
    // b = d / (1 + thickness) - n / (f - n) * (thickness / (1 + thickness))
    // b = d * k_s + k_b
    const float thickness = 0.0016;
    const float near = 0.3;
    const float far = 100.0;
    const float thickness_scale = 1.0 / (1.0 + thickness);
    const float thickness_bias = -near / (far - near) * (thickness * thickness_scale);
    // Hi-Z 的最大 mip level
    const int maxMipLevel = 8;

    float3 rayPos;

    // 采样 Hi-Z 的层级
    int mipLevel = 0;
    // 步进次数
    int iterCount = 0;
    bool hit = false;
    bool miss = false;
    bool belowMip0 = false;

    while (!(hit || miss) && (t <= tMax) && (iterCount < _SsrIterLimit))
    {
        // 每次都是从起点步进 t 距离, 计算出此次步进后的终点在原始屏幕空间中的坐标位置
        rayPos = rayOrigin + t * rayDir;

        // 为了避免 rayPos 处于屏幕纹素的边缘, 对其应用一个微小的偏移量
        float2 sgnEdgeDist = round(rayPos.xy) - rayPos.xy;
        float2 satEdgeDist = clamp(raySign.xy * sgnEdgeDist + SSR_TRACE_EPS, 0, SSR_TRACE_EPS);
        rayPos.xy += raySign.xy * satEdgeDist;

        // 根据当前 Hi-Z 层级, 计算出在此次步进终点采样 Hi-Z 能覆盖的原始屏幕空间中最远的屏幕坐标位置
        float4 bounds;
        int2 mipCoord  = (int2)rayPos.xy >> mipLevel;
        bounds.xy = (mipCoord + rayStep) << mipLevel;
        // 采样当前 Hi-Z Mip 层级中的深度值, 可以理解为几何体表面深度值
        bounds.z = _DepthPyramidTexture.Load(int3(mipCoord, mipLevel)).r;
        // 应用几何体厚度后的深度值, 可以理解为几何体背面深度值
        bounds.w = bounds.z * thickness_scale + thickness_bias;

        // 是否在几何体表面以下
        bool belowFloor = rayPos.z < bounds.z;
        // 是否在几何体背面以上
        bool aboveBase = rayPos.z >= bounds.w;
        // 在几何体内部
        bool insideFloor = belowFloor && aboveBase;

        // 在原始屏幕空间上, 此次步进终点能覆盖的最远坐标位置相对于起点的距离变化
        float4 dist = bounds * rcpRayDir.xyzz - (rayOrigin.xyzz * rcpRayDir.xyzz);
        float distWall  = min(dist.x, dist.y);
        float distFloor = dist.z;
        // 当此次步进的步长 t 与 t <= distFloor <= distWall , 则认为交点正好在几何体表面
        bool hitFloor = (t <= distFloor) && (distFloor <= distWall);

        // 如果 belowMip0 为 true , 表示上次步进时, Mip 已经为 0 且没有找到交点, 则认为没有找到交点, 跳出步进
        miss = belowMip0;
        // 当前在 Mip 0 且交点在几何体表面或几何体内部则认为找到交点, 跳出步进
        hit = (mipLevel == 0) && (hitFloor || insideFloor);

        // 当反射光线深度在几何体表面以下, 且此时采样的 Hi-Z Mip 为 0 , 则 belowMip0 为 true , belowMip0 默认为 false
        belowMip0 = (mipLevel == 0) && belowFloor;

        // 如果交点在几何体表面, 则减小步长, 使用 distFloor 作为下一次步进的步长, 并在后续对 Hi-Z Mip -1
        // 如果 Hi-Z Mip 不为 0 且反射光线深度值小于物体表面深度, 则保持当前步长并在后续对 Hi-Z Mip -1
        // 否则, 增大步长, 使用 distWall 作为下一次步长, 并在后续对 Hi-Z Mip +1
        t = hitFloor ? distFloor : (((mipLevel != 0) && belowFloor) ? t : distWall);

        // 交点在几何体表面, 或者在几何体表面以下, 则下次步进采样低 1 级的 Hi-Z , 否则下次步进采样高 1 级的 Hi-Z
        mipLevel += (hitFloor || belowFloor) ? -1 : 1;
        mipLevel  = clamp(mipLevel, 0, maxMipLevel);

        iterCount++;
    }

    // 确定最终是否找到交点
    miss = miss || rayPos.z == 0;
    hit  = hit && !miss;

    if (hit)
    {
        // 根据最终反射光线屏幕空间坐标计算 uv 坐标
        float2 hitPositionNDC = floor(rayPos.xy) * _ScreenSize.zw + (0.5 * _ScreenSize.zw);

        if (max(hitPositionNDC.x, hitPositionNDC.y) == 0.0)
        {
            // Miss.
            _SsrLightingTexture[positionSS] = float4(0.0, 0.0, 0.0, 0.0);
            return;
        }

        // 重投影到上一帧的屏幕 uv 坐标
        float4 q = mul(_HistoryReprojection, float4(hitPositionNDC, deviceDepth, 1.0));
        float2 historyUV = (q.xy * (1.0 / q.w)) * 0.5 + 0.5;

        if (any(historyUV < float2(0.0, 0.0)) || any(historyUV > float2(1.0, 1.0)))
        {
            // Off-Screen.
            _SsrLightingTexture[positionSS] = float4(0.0, 0.0, 0.0, 0.0);
            return;
        }

        // 采样上一帧的颜色缓冲
        float3 color = _SsrHistoryColorTexture.SampleLevel(sampler_LinearClamp, historyUV, 0).rgb;

        // 检查上一帧中的颜色缓冲中无效的值
        uint3 intCol = asuint(color);
        bool isPosFin = Max3(intCol.r, intCol.g, intCol.b) < 0x7F800000;

        // 对处于屏幕边缘的反射做过度, 避免反射结果贴图直接被截断
        const float screenFadeDistance = 0.1;
        const float _SsrEdgeFadeRcpLength = min(1.0 / screenFadeDistance, FLT_MAX);
        float opacity = EdgeOfScreenFade(historyUV, _SsrEdgeFadeRcpLength);

        color = isPosFin ? color : 0.0;
        opacity = isPosFin ? opacity : 0.0;

        _SsrLightingTexture[positionSS] = float4(color, 1.0) * opacity;
    }
    else
    {
        _SsrLightingTexture[positionSS] = float4(0.0, 0.0, 0.0, 0.0);
    }
}
